{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RemyaVKarthikeyan/AA-Stagecoach-Project/blob/main/18_07_2024_13_43_Cumulative_prediction_no_of_buses_observed_AWT_dataframe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "with dataframe showing the number of buses observed in an hr and AWT calculation"
      ],
      "metadata": {
        "id": "j9d1YrLY32J8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aFDa5DVspn8"
      },
      "source": [
        "18/07/2024"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EBt4bRPk40tG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "# Function to normalize stop names\n",
        "def normalize_stop_name(name):\n",
        "    return ' '.join(name.lower().split())\n",
        "\n",
        "# Modify this path accordingly\n",
        "file_path = '/content/QSI points.xlsx'\n",
        "\n",
        "# Read the Excel file into a DataFrame\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Get the lineID from the user\n",
        "lineID = input(\"Please enter the lineID: \")\n",
        "\n",
        "# Check if the necessary column is present\n",
        "if 'Route_Dir_QSI_No' not in df.columns:\n",
        "    print(\"The 'Route_Dir_QSI_No' column is not present in the provided file.\")\n",
        "else:\n",
        "    # Convert the lineID to uppercase to ensure case-insensitivity\n",
        "    lineID = lineID.upper()\n",
        "\n",
        "    # Convert the 'Route_Dir_QSI_No' column to uppercase for comparison\n",
        "    df['Route_Dir_QSI_No'] = df['Route_Dir_QSI_No'].str.upper()\n",
        "\n",
        "    # Normalize the stop names in the DataFrame\n",
        "    df['STOP_NAME'] = df['STOP_NAME'].apply(normalize_stop_name)\n",
        "\n",
        "    # Create regular expressions for filtering\n",
        "    pattern_A = f\"^{lineID}_A\\\\d+$\"  # Regular expression for lineID_A**\n",
        "    pattern_B = f\"^{lineID}_B\\\\d+$\"  # Regular expression for lineID_B**\n",
        "\n",
        "    # Filter rows where the 'Route_Dir_QSI_No' column matches the pattern\n",
        "    filtered_df_A = df[df['Route_Dir_QSI_No'].str.match(pattern_A, na=False)][['Route_Dir_QSI_No', 'STOP_NAME']]\n",
        "    filtered_df_B = df[df['Route_Dir_QSI_No'].str.match(pattern_B, na=False)][['Route_Dir_QSI_No', 'STOP_NAME']]\n",
        "\n",
        "    # Function to fetch and process route sequence data from TfL API\n",
        "    def fetch_and_process_route_data(route_type, pattern, filtered_df):\n",
        "        api_url = f\"https://api.tfl.gov.uk/Line/{lineID}/Route/Sequence/{route_type}\"\n",
        "        response = requests.get(api_url)\n",
        "\n",
        "        results_list = []\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            route_data = response.json()\n",
        "\n",
        "            # Iterate through each stop in the route data\n",
        "            for stop in route_data['stopPointSequences'][0]['stopPoint']:\n",
        "                stop_name_api = normalize_stop_name(stop['name'])\n",
        "                stop_id = stop['id']\n",
        "\n",
        "                # Check if the stop_name_api exists in the filtered DataFrame for the correct direction\n",
        "                matched_row = filtered_df[(filtered_df['STOP_NAME'] == stop_name_api) &\n",
        "                                          (filtered_df['Route_Dir_QSI_No'].str.match(pattern))]\n",
        "\n",
        "                if not matched_row.empty:\n",
        "                    route_dir_qsi_no = matched_row.iloc[0]['Route_Dir_QSI_No']\n",
        "                    results_list.append({\n",
        "                        'Route_Dir_QSI_No': route_dir_qsi_no,\n",
        "                        'STOP_Name': stop['name'],\n",
        "                        'ID': stop_id\n",
        "                    })\n",
        "                else:\n",
        "                    # If exact match not found, try partial matching based on words before and after '/'\n",
        "                    api_stop_name_parts = stop_name_api.split('/')\n",
        "                    for index, row in filtered_df.iterrows():\n",
        "                        df_stop_name_parts = row['STOP_NAME'].split('/')\n",
        "                        for api_part in api_stop_name_parts:\n",
        "                            for df_part in df_stop_name_parts:\n",
        "                                if SequenceMatcher(None, df_part.strip(), api_part.strip()).ratio() > 0.8:\n",
        "                                    matched_row = pd.DataFrame([row])\n",
        "                                    break\n",
        "                            if not matched_row.empty:\n",
        "                                break\n",
        "                        if not matched_row.empty:\n",
        "                            break\n",
        "\n",
        "                    if not matched_row.empty:\n",
        "                        route_dir_qsi_no = matched_row.iloc[0]['Route_Dir_QSI_No']\n",
        "                        results_list.append({\n",
        "                            'Route_Dir_QSI_No': route_dir_qsi_no,\n",
        "                            'STOP_Name': stop['name'],\n",
        "                            'ID': stop_id\n",
        "                        })\n",
        "        else:\n",
        "            print(f\"Failed to fetch route sequence data from TfL API for {route_type} route. Status code: {response.status_code}\")\n",
        "\n",
        "        return results_list\n",
        "\n",
        "    # Fetch and process outbound route data for _A**\n",
        "    matched_results_A = fetch_and_process_route_data('outbound', pattern_A, filtered_df_A)\n",
        "\n",
        "    # Fetch and process inbound route data for _B**\n",
        "    matched_results_B = fetch_and_process_route_data('inbound', pattern_B, filtered_df_B)\n",
        "\n",
        "    # Create DataFrames from the matched results for each direction\n",
        "    matched_results_df_A = pd.DataFrame(matched_results_A)\n",
        "    matched_results_df_B = pd.DataFrame(matched_results_B)\n",
        "\n",
        "    # Function to remove partial matches if exact matches are found\n",
        "    def remove_partial_matches(exact_df, matched_df):\n",
        "        for index, row in exact_df.iterrows():\n",
        "            exact_stop_name = row['STOP_NAME']\n",
        "            route_dir_qsi_no = row['Route_Dir_QSI_No']\n",
        "            # Find exact matches in matched_df\n",
        "            exact_matches = matched_df[(matched_df['Route_Dir_QSI_No'] == route_dir_qsi_no) &\n",
        "                                       (matched_df['STOP_Name'].apply(normalize_stop_name) == exact_stop_name)]\n",
        "            if not exact_matches.empty:\n",
        "                # Remove partial matches\n",
        "                matched_df = matched_df[~((matched_df['Route_Dir_QSI_No'] == route_dir_qsi_no) &\n",
        "                                          (matched_df['STOP_Name'].apply(normalize_stop_name) != exact_stop_name))]\n",
        "        return matched_df\n",
        "\n",
        "    # Remove partial matches for direction A\n",
        "    matched_results_df_A = remove_partial_matches(filtered_df_A, matched_results_df_A)\n",
        "\n",
        "    # Remove partial matches for direction B\n",
        "    matched_results_df_B = remove_partial_matches(filtered_df_B, matched_results_df_B)\n",
        "\n",
        "    # Remove duplicate stop names with the same Route_Dir_QSI_No and different IDs\n",
        "    matched_results_df_A = matched_results_df_A.drop_duplicates(subset=['Route_Dir_QSI_No', 'STOP_Name'], keep='first')\n",
        "    matched_results_df_B = matched_results_df_B.drop_duplicates(subset=['Route_Dir_QSI_No', 'STOP_Name'], keep='first')\n",
        "\n",
        "    # Print the matched results for direction A\n",
        "    print(f\"\\n\\n\\033[1m\\033[4mQSI stop points for direction {lineID}_A\\033[0m\\n\")\n",
        "    matched_results_df_A = matched_results_df_A[matched_results_df_A['Route_Dir_QSI_No'].str.match(pattern_A)]\n",
        "    print(matched_results_df_A[['Route_Dir_QSI_No', 'STOP_Name', 'ID']])\n",
        "\n",
        "    # Print the matched results for direction B\n",
        "    print(f\"\\n\\n\\033[1m\\033[4mQSI stop points for direction {lineID}_B\\033[0m\\n\")\n",
        "    matched_results_df_B = matched_results_df_B[matched_results_df_B['Route_Dir_QSI_No'].str.match(pattern_B)]\n",
        "    print(matched_results_df_B[['Route_Dir_QSI_No', 'STOP_Name', 'ID']])\n",
        "\n",
        "    # Concatenate the matched results DataFrames for directions A and B\n",
        "    combined_df = pd.concat([matched_results_df_A, matched_results_df_B], ignore_index=True)\n",
        "\n",
        "    # Display the combined DataFrame\n",
        "    print(\"\\n\\n\\033[1m\\033[4mCombined QSI stop points for directions A and B\\033[0m\\n\")\n",
        "    print(combined_df[['Route_Dir_QSI_No', 'STOP_Name', 'ID']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7da96c0-97cb-4442-a8b0-62001bd9d700",
        "id": "7YH0gajXuMuI"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter the lineID: 145\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[4mQSI stop points for direction 145_A\u001b[0m\n",
            "\n",
            "   Route_Dir_QSI_No                         STOP_Name           ID\n",
            "0            145_A1                     Dagenham Asda   490005922N\n",
            "1            145_A2   Ballards Road / Princess Parade  490003565E2\n",
            "2            145_A3                          Heathway   490008035K\n",
            "3            145_A4                 Becontree Station   490000019A\n",
            "4            145_A5   Martins Corner / Valence Avenue   490009704C\n",
            "5            145_A6                   Longbridge Road   490009293N\n",
            "6            145_A7                   Hainault Street   490007657W\n",
            "7            145_A8                  High Road Ilford   490008470M\n",
            "8            145_A9                 Redbridge Station   490015554C\n",
            "10          145_A10  Wanstead Station  / George Green   490015125C\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[4mQSI stop points for direction 145_B\u001b[0m\n",
            "\n",
            "   Route_Dir_QSI_No                        STOP_Name          ID\n",
            "0            145_B1              Leytonstone Station  490000137T\n",
            "1            145_B2                 Wanstead Station  490015125B\n",
            "2            145_B3                Redbridge Station  490006659E\n",
            "3            145_B4                   Ilford Station  490001157K\n",
            "4            145_B5                  Hainault Street  490007657T\n",
            "5            145_B6                  Longbridge Road  490009293S\n",
            "6            145_B7  Martins Corner / Valence Avenue  490009704A\n",
            "7            145_B8                Becontree Station  490000019B\n",
            "8            145_B9                         Heathway  490008035F\n",
            "10          145_B10                    Ballards Road  490003565W\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[4mCombined QSI stop points for directions A and B\u001b[0m\n",
            "\n",
            "   Route_Dir_QSI_No                         STOP_Name           ID\n",
            "0            145_A1                     Dagenham Asda   490005922N\n",
            "1            145_A2   Ballards Road / Princess Parade  490003565E2\n",
            "2            145_A3                          Heathway   490008035K\n",
            "3            145_A4                 Becontree Station   490000019A\n",
            "4            145_A5   Martins Corner / Valence Avenue   490009704C\n",
            "5            145_A6                   Longbridge Road   490009293N\n",
            "6            145_A7                   Hainault Street   490007657W\n",
            "7            145_A8                  High Road Ilford   490008470M\n",
            "8            145_A9                 Redbridge Station   490015554C\n",
            "9           145_A10  Wanstead Station  / George Green   490015125C\n",
            "10           145_B1               Leytonstone Station   490000137T\n",
            "11           145_B2                  Wanstead Station   490015125B\n",
            "12           145_B3                 Redbridge Station   490006659E\n",
            "13           145_B4                    Ilford Station   490001157K\n",
            "14           145_B5                   Hainault Street   490007657T\n",
            "15           145_B6                   Longbridge Road   490009293S\n",
            "16           145_B7   Martins Corner / Valence Avenue   490009704A\n",
            "17           145_B8                 Becontree Station   490000019B\n",
            "18           145_B9                          Heathway   490008035F\n",
            "19          145_B10                     Ballards Road   490003565W\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import pytz\n",
        "import time\n",
        "\n",
        "# Function to fetch arrival predictions\n",
        "def fetch_arrival_predictions(line_id, stop_point_id, direction):\n",
        "    try:\n",
        "        base_url = f\"https://api.tfl.gov.uk/Line/{line_id}/Arrivals/{stop_point_id}\"\n",
        "        params = {'direction': direction}\n",
        "        response = requests.get(base_url, params=params)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        if len(data) == 0:\n",
        "            return pd.DataFrame(), None  # No data available\n",
        "        station_name = data[0]['stationName']\n",
        "        predictions = []\n",
        "        for item in data:\n",
        "            arrival_time = datetime.strptime(item['expectedArrival'], '%Y-%m-%dT%H:%M:%SZ')\n",
        "            arrival_time_bst = arrival_time + timedelta(hours=1)\n",
        "            predictions.append({\n",
        "                'Line': item['lineName'],\n",
        "                'Vehicle ID': item['vehicleId'],\n",
        "                'Stop Point': stop_point_id,\n",
        "                'Direction': direction,\n",
        "                'Expected Arrival (BST)': arrival_time_bst,\n",
        "                'Expected Arrival (HM)': arrival_time_bst.strftime('%H:%M')\n",
        "            })\n",
        "        df = pd.DataFrame(predictions)\n",
        "        df = df.sort_values(by='Expected Arrival (BST)', ascending=True)\n",
        "        df['Expected Arrival (BST)'] = pd.to_datetime(df['Expected Arrival (BST)'])  # Convert to datetime\n",
        "        df['Expected Arrival (HM)'] = pd.to_datetime(df['Expected Arrival (HM)'], format='%H:%M')\n",
        "        df['Gap'] = df['Expected Arrival (HM)'].diff().fillna(pd.Timedelta(seconds=0)).dt.total_seconds() / 60\n",
        "        df['2_Gap'] = (df['Gap'] * 2).round(2)\n",
        "        df['Gap_Sq'] = (df['Gap'] * df['Gap']).round(2)\n",
        "        return df, station_name\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching data: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def main():\n",
        "    # Assuming combined_df is generated from the previous cell\n",
        "    # combined_df should have columns like 'Route_Dir_QSI_No', 'STOP_Name', 'ID'\n",
        "\n",
        "    # Example dictionary to hold cumulative dataframes for each stop point\n",
        "    cumulative_dataframes = {}\n",
        "\n",
        "    # Dictionary to hold the number of buses observed per stop point\n",
        "    buses_observed = {}\n",
        "\n",
        "    # Loop through unique stop points in combined_df\n",
        "    for index, row in combined_df.iterrows():\n",
        "        stop_point_id = row['ID']\n",
        "        direction = 'outbound' if row['Route_Dir_QSI_No'].startswith(lineID + '_A') else 'inbound'\n",
        "\n",
        "        cumulative_df = pd.DataFrame(columns=[\n",
        "            'Line', 'Vehicle ID', 'Stop Point', 'Direction',\n",
        "            'Expected Arrival (BST)', 'Expected Arrival (HM)',\n",
        "            'Gap', '2_Gap', 'Gap_Sq'\n",
        "        ])\n",
        "\n",
        "        cumulative_dataframes[stop_point_id] = cumulative_df  # Initialize cumulative dataframe\n",
        "        buses_observed[stop_point_id] = 0  # Initialize number of buses observed to 0\n",
        "\n",
        "    while True:\n",
        "        for stop_point_id, cumulative_df in cumulative_dataframes.items():\n",
        "            direction = 'outbound' if combined_df[combined_df['ID'] == stop_point_id]['Route_Dir_QSI_No'].iloc[0].startswith(lineID + '_A') else 'inbound'\n",
        "\n",
        "            arrival_predictions_df, station_name = fetch_arrival_predictions(lineID, stop_point_id, direction)\n",
        "\n",
        "            if arrival_predictions_df is not None and not arrival_predictions_df.empty:\n",
        "                current_hour = datetime.now(pytz.timezone('Europe/London')).hour\n",
        "\n",
        "                for _, row in arrival_predictions_df.iterrows():\n",
        "                    vehicle_id = row['Vehicle ID']\n",
        "                    expected_hour = row['Expected Arrival (BST)'].hour\n",
        "\n",
        "                    mask = cumulative_df['Vehicle ID'] == vehicle_id\n",
        "\n",
        "                    if cumulative_df[mask].empty:\n",
        "                        cumulative_df = pd.concat([cumulative_df, row.to_frame().T], ignore_index=True)\n",
        "                    else:\n",
        "                        existing_hour = cumulative_df.loc[mask, 'Expected Arrival (BST)'].iloc[0].hour\n",
        "\n",
        "                        if expected_hour > existing_hour + 1:\n",
        "                            cumulative_df = pd.concat([cumulative_df, row.to_frame().T], ignore_index=True)\n",
        "                        else:\n",
        "                            cumulative_df.loc[mask, ['Line', 'Stop Point', 'Direction', 'Expected Arrival (BST)', 'Expected Arrival (HM)', 'Gap', '2_Gap', 'Gap_Sq']] = row[['Line', 'Stop Point', 'Direction', 'Expected Arrival (BST)', 'Expected Arrival (HM)', 'Gap', '2_Gap', 'Gap_Sq']].values\n",
        "\n",
        "                cumulative_df = cumulative_df.sort_values(by='Expected Arrival (BST)', ascending=True)\n",
        "                cumulative_df['Expected Arrival (BST)'] = pd.to_datetime(cumulative_df['Expected Arrival (BST)'])\n",
        "                cumulative_df['Expected Arrival (HM)'] = pd.to_datetime(cumulative_df['Expected Arrival (HM)'], format='%H:%M')\n",
        "\n",
        "                cumulative_df['Gap'] = cumulative_df['Expected Arrival (HM)'].diff().fillna(pd.Timedelta(seconds=0)).dt.total_seconds() / 60\n",
        "                cumulative_df['2_Gap'] = (cumulative_df['Gap'] * 2).round(2)\n",
        "                cumulative_df['Gap_Sq'] = (cumulative_df['Gap'] * cumulative_df['Gap']).round(2)\n",
        "\n",
        "                # Update number of buses observed in the current hour\n",
        "                num_buses_observed = cumulative_df[cumulative_df['Expected Arrival (BST)'].dt.hour == current_hour].shape[0]\n",
        "                total_Gap_Sq = cumulative_df[cumulative_df['Expected Arrival (BST)'].dt.hour == current_hour]['Gap_Sq'].sum()\n",
        "                total_2_Gap = cumulative_df[cumulative_df['Expected Arrival (BST)'].dt.hour == current_hour]['2_Gap'].sum()\n",
        "                AWT = round(total_Gap_Sq / total_2_Gap, 2) if total_2_Gap > 0 else 0\n",
        "                buses_observed[stop_point_id] = (num_buses_observed, total_Gap_Sq,total_2_Gap,AWT)\n",
        "\n",
        "                print(f\"\\nArrival Predictions for stop point {stop_point_id} ({station_name}):\")\n",
        "                print(arrival_predictions_df.to_string(index=False))\n",
        "                print(\"\\nCumulative DataFrame:\")\n",
        "                print(cumulative_df.to_string(index=False))\n",
        "                print(f\"\\nNumber of buses observed in the current hour: {num_buses_observed}\")\n",
        "            else:\n",
        "                print(\"No arrival predictions available.\")\n",
        "\n",
        "            print(\"Refreshing data in 30 seconds...\\n\")\n",
        "            time.sleep(30)\n",
        "\n",
        "            # Update cumulative dataframe in dictionary\n",
        "            cumulative_dataframes[stop_point_id] = cumulative_df\n",
        "\n",
        "        # Create DataFrame to show number of buses observed for each stop point\n",
        "        buses_observed_df = pd.DataFrame(list(buses_observed.items()), columns=['Stop Point', 'Num of Buses Observed','total_Gap_Sq','total_2_Gap','AWT'])\n",
        "\n",
        "        print(\"\\nNumber of Buses Observed DataFrame:\")\n",
        "        print(buses_observed_df)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e3ace42f-a500-4aa3-e0d9-e934723d15da",
        "id": "zvXqifnnBktM"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No arrival predictions available.\n",
            "Refreshing data in 30 seconds...\n",
            "\n",
            "\n",
            "Arrival Predictions for stop point 490013513S (Stewart Street):\n",
            "Line Vehicle ID Stop Point Direction Expected Arrival (BST) Expected Arrival (HM)  Gap  2_Gap  Gap_Sq\n",
            "  D7    LX11BJU 490013513S  outbound    2024-07-18 15:03:05   1900-01-01 15:03:00  0.0    0.0     0.0\n",
            "\n",
            "Cumulative DataFrame:\n",
            "Line Vehicle ID Stop Point Direction Expected Arrival (BST) Expected Arrival (HM)  Gap  2_Gap  Gap_Sq\n",
            "  D7    LX11BJU 490013513S  outbound    2024-07-18 15:03:05   1900-01-01 15:03:00  0.0    0.0     0.0\n",
            "\n",
            "Number of buses observed in the current hour: 0\n",
            "Refreshing data in 30 seconds...\n",
            "\n",
            "No arrival predictions available.\n",
            "Refreshing data in 30 seconds...\n",
            "\n",
            "\n",
            "Arrival Predictions for stop point 490006092N (Arnhem Wharf Primary School):\n",
            "Line Vehicle ID Stop Point Direction Expected Arrival (BST) Expected Arrival (HM)  Gap  2_Gap  Gap_Sq\n",
            "  D7    LX12DDE 490006092N  outbound    2024-07-18 15:02:07   1900-01-01 15:02:00  0.0    0.0     0.0\n",
            "\n",
            "Cumulative DataFrame:\n",
            "Line Vehicle ID Stop Point Direction Expected Arrival (BST) Expected Arrival (HM)  Gap  2_Gap  Gap_Sq\n",
            "  D7    LX12DDE 490006092N  outbound    2024-07-18 15:02:07   1900-01-01 15:02:00  0.0    0.0     0.0\n",
            "\n",
            "Number of buses observed in the current hour: 1\n",
            "Refreshing data in 30 seconds...\n",
            "\n",
            "\n",
            "Arrival Predictions for stop point 490004584N (East India Dock Road):\n",
            "Line Vehicle ID Stop Point Direction Expected Arrival (BST) Expected Arrival (HM)  Gap  2_Gap  Gap_Sq\n",
            "  D7    LX11BJV 490004584N  outbound    2024-07-18 15:05:56   1900-01-01 15:05:00  0.0    0.0     0.0\n",
            "\n",
            "Cumulative DataFrame:\n",
            "Line Vehicle ID Stop Point Direction Expected Arrival (BST) Expected Arrival (HM)  Gap  2_Gap  Gap_Sq\n",
            "  D7    LX11BJV 490004584N  outbound    2024-07-18 15:05:56   1900-01-01 15:05:00  0.0    0.0     0.0\n",
            "\n",
            "Number of buses observed in the current hour: 1\n",
            "Refreshing data in 30 seconds...\n",
            "\n",
            "\n",
            "Arrival Predictions for stop point 490015151H (Mile End Station / Bow Road):\n",
            "Line Vehicle ID Stop Point Direction Expected Arrival (BST) Expected Arrival (HM)  Gap  2_Gap  Gap_Sq\n",
            "  D7    LX11BFF 490015151H   inbound    2024-07-18 15:02:55   1900-01-01 15:02:00  0.0    0.0     0.0\n",
            "\n",
            "Cumulative DataFrame:\n",
            "Line Vehicle ID Stop Point Direction Expected Arrival (BST) Expected Arrival (HM)  Gap  2_Gap  Gap_Sq\n",
            "  D7    LX11BFF 490015151H   inbound    2024-07-18 15:02:55   1900-01-01 15:02:00  0.0    0.0     0.0\n",
            "\n",
            "Number of buses observed in the current hour: 1\n",
            "Refreshing data in 30 seconds...\n",
            "\n",
            "No arrival predictions available.\n",
            "Refreshing data in 30 seconds...\n",
            "\n",
            "\n",
            "Arrival Predictions for stop point 490000038F (Canary Wharf Station):\n",
            "Line Vehicle ID Stop Point Direction Expected Arrival (BST) Expected Arrival (HM)  Gap  2_Gap  Gap_Sq\n",
            "  D7    LX61DBV 490000038F   inbound    2024-07-18 15:04:23   1900-01-01 15:04:00  0.0    0.0     0.0\n",
            "\n",
            "Cumulative DataFrame:\n",
            "Line Vehicle ID Stop Point Direction Expected Arrival (BST) Expected Arrival (HM)  Gap  2_Gap  Gap_Sq\n",
            "  D7    LX61DBV 490000038F   inbound    2024-07-18 15:04:23   1900-01-01 15:04:00  0.0    0.0     0.0\n",
            "\n",
            "Number of buses observed in the current hour: 1\n",
            "Refreshing data in 30 seconds...\n",
            "\n",
            "\n",
            "Arrival Predictions for stop point 490006092S (Arnhem Wharf Primary School):\n",
            "Line Vehicle ID Stop Point Direction Expected Arrival (BST) Expected Arrival (HM)  Gap  2_Gap  Gap_Sq\n",
            "  D7    LX61DBO 490006092S   inbound    2024-07-18 15:03:09   1900-01-01 15:03:00  0.0    0.0     0.0\n",
            "\n",
            "Cumulative DataFrame:\n",
            "Line Vehicle ID Stop Point Direction Expected Arrival (BST) Expected Arrival (HM)  Gap  2_Gap  Gap_Sq\n",
            "  D7    LX61DBO 490006092S   inbound    2024-07-18 15:03:09   1900-01-01 15:03:00  0.0    0.0     0.0\n",
            "\n",
            "Number of buses observed in the current hour: 1\n",
            "Refreshing data in 30 seconds...\n",
            "\n",
            "No arrival predictions available.\n",
            "Refreshing data in 30 seconds...\n",
            "\n",
            "No arrival predictions available.\n",
            "Refreshing data in 30 seconds...\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "5 columns passed, passed data had 2 columns",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    933\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_or_indexify_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_validate_or_indexify_columns\u001b[0;34m(content, columns)\u001b[0m\n\u001b[1;32m    980\u001b[0m             \u001b[0;31m# caller's responsibility to check for this...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m             raise AssertionError(\n\u001b[0m\u001b[1;32m    982\u001b[0m                 \u001b[0;34mf\"{len(columns)} columns passed, passed data had \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: 5 columns passed, passed data had 2 columns",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-657a0507cd70>\u001b[0m in \u001b[0;36m<cell line: 126>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-657a0507cd70>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# Create DataFrame to show number of buses observed for each stop point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mbuses_observed_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuses_observed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Stop Point'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Num of Buses Observed'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'total_Gap_Sq'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'total_2_Gap'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'AWT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nNumber of Buses Observed DataFrame:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    780\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m                         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m                     arrays, columns, index = nested_data_to_arrays(\n\u001b[0m\u001b[1;32m    783\u001b[0m                         \u001b[0;31m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m                         \u001b[0;31m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m     \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_list_to_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m     \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_finalize_columns_and_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    935\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m         \u001b[0;31m# GH#26429 do not raise user-facing AssertionError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: 5 columns passed, passed data had 2 columns"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import pytz\n",
        "import time\n",
        "\n",
        "# Function to fetch arrival predictions\n",
        "def fetch_arrival_predictions(line_id, stop_point_id, direction):\n",
        "    try:\n",
        "        base_url = f\"https://api.tfl.gov.uk/Line/{line_id}/Arrivals/{stop_point_id}\"\n",
        "        params = {'direction': direction}\n",
        "        response = requests.get(base_url, params=params)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        if len(data) == 0:\n",
        "            return pd.DataFrame(), None  # No data available\n",
        "        station_name = data[0]['stationName']\n",
        "        predictions = []\n",
        "        for item in data:\n",
        "            arrival_time = datetime.strptime(item['expectedArrival'], '%Y-%m-%dT%H:%M:%SZ')\n",
        "            arrival_time_bst = arrival_time + timedelta(hours=1)\n",
        "            predictions.append({\n",
        "                'Line': item['lineName'],\n",
        "                'Vehicle ID': item['vehicleId'],\n",
        "                'Stop Point': stop_point_id,\n",
        "                'Direction': direction,\n",
        "                'Expected Arrival (BST)': arrival_time_bst,\n",
        "                'Expected Arrival (HM)': arrival_time_bst.strftime('%H:%M')\n",
        "            })\n",
        "        df = pd.DataFrame(predictions)\n",
        "        df = df.sort_values(by='Expected Arrival (BST)', ascending=True)\n",
        "        df['Expected Arrival (BST)'] = pd.to_datetime(df['Expected Arrival (BST)'])  # Convert to datetime\n",
        "        df['Expected Arrival (HM)'] = pd.to_datetime(df['Expected Arrival (HM)'], format='%H:%M')\n",
        "        df['Gap'] = df['Expected Arrival (HM)'].diff().fillna(pd.Timedelta(seconds=0)).dt.total_seconds() / 60\n",
        "        df['2_Gap'] = (df['Gap'] * 2).round(2)\n",
        "        df['Gap_Sq'] = (df['Gap'] * df['Gap']).round(2)\n",
        "        return df, station_name\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching data: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def main():\n",
        "    # Assuming combined_df is generated from the previous cell\n",
        "    # combined_df should have columns like 'Route_Dir_QSI_No', 'STOP_Name', 'ID'\n",
        "\n",
        "    # Example dictionary to hold cumulative dataframes for each stop point\n",
        "    cumulative_dataframes = {}\n",
        "\n",
        "    # Dictionary to hold the number of buses observed per stop point\n",
        "    buses_observed = {}\n",
        "\n",
        "    # Loop through unique stop points in combined_df\n",
        "    for index, row in combined_df.iterrows():\n",
        "        stop_point_id = row['ID']\n",
        "        direction = 'outbound' if row['Route_Dir_QSI_No'].startswith(lineID + '_A') else 'inbound'\n",
        "\n",
        "        cumulative_df = pd.DataFrame(columns=[\n",
        "            'Line', 'Vehicle ID', 'Stop Point', 'Direction',\n",
        "            'Expected Arrival (BST)', 'Expected Arrival (HM)',\n",
        "            'Gap', '2_Gap', 'Gap_Sq'\n",
        "        ])\n",
        "\n",
        "        cumulative_dataframes[stop_point_id] = cumulative_df  # Initialize cumulative dataframe\n",
        "        buses_observed[stop_point_id] = (0, 0, 0, 0)  # Initialize with zero values\n",
        "\n",
        "    while True:\n",
        "        for stop_point_id, cumulative_df in cumulative_dataframes.items():\n",
        "            direction = 'outbound' if combined_df[combined_df['ID'] == stop_point_id]['Route_Dir_QSI_No'].iloc[0].startswith(lineID + '_A') else 'inbound'\n",
        "\n",
        "            arrival_predictions_df, station_name = fetch_arrival_predictions(lineID, stop_point_id, direction)\n",
        "\n",
        "            if arrival_predictions_df is not None and not arrival_predictions_df.empty:\n",
        "                current_hour = datetime.now(pytz.timezone('Europe/London')).hour\n",
        "\n",
        "                for _, row in arrival_predictions_df.iterrows():\n",
        "                    vehicle_id = row['Vehicle ID']\n",
        "                    expected_hour = row['Expected Arrival (BST)'].hour\n",
        "\n",
        "                    mask = cumulative_df['Vehicle ID'] == vehicle_id\n",
        "\n",
        "                    if cumulative_df[mask].empty:\n",
        "                        cumulative_df = pd.concat([cumulative_df, row.to_frame().T], ignore_index=True)\n",
        "                    else:\n",
        "                        existing_hour = cumulative_df.loc[mask, 'Expected Arrival (BST)'].iloc[0].hour\n",
        "\n",
        "                        if expected_hour > existing_hour + 1:\n",
        "                            cumulative_df = pd.concat([cumulative_df, row.to_frame().T], ignore_index=True)\n",
        "                        else:\n",
        "                            cumulative_df.loc[mask, ['Line', 'Stop Point', 'Direction', 'Expected Arrival (BST)', 'Expected Arrival (HM)', 'Gap', '2_Gap', 'Gap_Sq']] = row[['Line', 'Stop Point', 'Direction', 'Expected Arrival (BST)', 'Expected Arrival (HM)', 'Gap', '2_Gap', 'Gap_Sq']].values\n",
        "\n",
        "                cumulative_df = cumulative_df.sort_values(by='Expected Arrival (BST)', ascending=True)\n",
        "                cumulative_df['Expected Arrival (BST)'] = pd.to_datetime(cumulative_df['Expected Arrival (BST)'])\n",
        "                cumulative_df['Expected Arrival (HM)'] = pd.to_datetime(cumulative_df['Expected Arrival (HM)'], format='%H:%M')\n",
        "\n",
        "                cumulative_df['Gap'] = cumulative_df['Expected Arrival (HM)'].diff().fillna(pd.Timedelta(seconds=0)).dt.total_seconds() / 60\n",
        "                cumulative_df['2_Gap'] = (cumulative_df['Gap'] * 2).round(2)\n",
        "                cumulative_df['Gap_Sq'] = (cumulative_df['Gap'] * cumulative_df['Gap']).round(2)\n",
        "\n",
        "                # Update number of buses observed in the current hour\n",
        "                num_buses_observed = cumulative_df[cumulative_df['Expected Arrival (BST)'].dt.hour == current_hour].shape[0]\n",
        "                total_Gap_Sq = cumulative_df[cumulative_df['Expected Arrival (BST)'].dt.hour == current_hour]['Gap_Sq'].sum()\n",
        "                total_2_Gap = cumulative_df[cumulative_df['Expected Arrival (BST)'].dt.hour == current_hour]['2_Gap'].sum()\n",
        "                AWT = round(total_Gap_Sq / total_2_Gap, 2) if total_2_Gap > 0 else 0\n",
        "                buses_observed[stop_point_id] = (num_buses_observed, total_Gap_Sq, total_2_Gap, AWT)\n",
        "\n",
        "                print(f\"\\nArrival Predictions for stop point {stop_point_id} ({station_name}):\")\n",
        "                print(arrival_predictions_df.to_string(index=False))\n",
        "                print(\"\\nCumulative DataFrame:\")\n",
        "                print(cumulative_df.to_string(index=False))\n",
        "                print(f\"\\nNumber of buses observed in the current hour: {num_buses_observed}\")\n",
        "            else:\n",
        "                print(\"No arrival predictions available.\")\n",
        "\n",
        "            print(\"Refreshing data in 30 seconds...\\n\")\n",
        "            time.sleep(30)\n",
        "\n",
        "            # Update cumulative dataframe in dictionary\n",
        "            cumulative_dataframes[stop_point_id] = cumulative_df\n",
        "\n",
        "        # Create DataFrame to show number of buses observed for each stop point\n",
        "        buses_observed_df = pd.DataFrame(list(buses_observed.items()), columns=['Stop Point', 'Metrics'])\n",
        "\n",
        "        # Split 'Metrics' into separate columns\n",
        "        buses_observed_df[['Num of Buses Observed', 'Total Gap Sq', 'Total 2 Gap', 'AWT']] = pd.DataFrame(\n",
        "            buses_observed_df['Metrics'].tolist(), index=buses_observed_df.index\n",
        "        )\n",
        "\n",
        "        # Drop the 'Metrics' column\n",
        "        buses_observed_df.drop(columns=['Metrics'], inplace=True)\n",
        "\n",
        "        print(\"\\nNumber of Buses Observed DataFrame:\")\n",
        "        print(buses_observed_df)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ztc3BRQgLAgG",
        "outputId": "b9151b26-b46f-4d3f-fe09-fc105c0b9979"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No arrival predictions available.\n",
            "Refreshing data in 30 seconds...\n",
            "\n",
            "No arrival predictions available.\n",
            "Refreshing data in 30 seconds...\n",
            "\n",
            "No arrival predictions available.\n",
            "Refreshing data in 30 seconds...\n",
            "\n",
            "No arrival predictions available.\n",
            "Refreshing data in 30 seconds...\n",
            "\n",
            "No arrival predictions available.\n",
            "Refreshing data in 30 seconds...\n",
            "\n",
            "No arrival predictions available.\n",
            "Refreshing data in 30 seconds...\n",
            "\n",
            "No arrival predictions available.\n",
            "Refreshing data in 30 seconds...\n",
            "\n",
            "No arrival predictions available.\n",
            "Refreshing data in 30 seconds...\n",
            "\n",
            "No arrival predictions available.\n",
            "Refreshing data in 30 seconds...\n",
            "\n",
            "No arrival predictions available.\n",
            "Refreshing data in 30 seconds...\n",
            "\n",
            "No arrival predictions available.\n",
            "Refreshing data in 30 seconds...\n",
            "\n",
            "No arrival predictions available.\n",
            "Refreshing data in 30 seconds...\n",
            "\n",
            "No arrival predictions available.\n",
            "Refreshing data in 30 seconds...\n",
            "\n",
            "No arrival predictions available.\n",
            "Refreshing data in 30 seconds...\n",
            "\n",
            "No arrival predictions available.\n",
            "Refreshing data in 30 seconds...\n",
            "\n",
            "No arrival predictions available.\n",
            "Refreshing data in 30 seconds...\n",
            "\n",
            "No arrival predictions available.\n",
            "Refreshing data in 30 seconds...\n",
            "\n",
            "No arrival predictions available.\n",
            "Refreshing data in 30 seconds...\n",
            "\n",
            "No arrival predictions available.\n",
            "Refreshing data in 30 seconds...\n",
            "\n",
            "No arrival predictions available.\n",
            "Refreshing data in 30 seconds...\n",
            "\n",
            "\n",
            "Number of Buses Observed DataFrame:\n",
            "     Stop Point  Num of Buses Observed  Total Gap Sq  Total 2 Gap  AWT\n",
            "0    490005922N                      0             0            0    0\n",
            "1   490003565E2                      0             0            0    0\n",
            "2    490008035K                      0             0            0    0\n",
            "3    490000019A                      0             0            0    0\n",
            "4    490009704C                      0             0            0    0\n",
            "5    490009293N                      0             0            0    0\n",
            "6    490007657W                      0             0            0    0\n",
            "7    490008470M                      0             0            0    0\n",
            "8    490015554C                      0             0            0    0\n",
            "9    490015125C                      0             0            0    0\n",
            "10   490000137T                      0             0            0    0\n",
            "11   490015125B                      0             0            0    0\n",
            "12   490006659E                      0             0            0    0\n",
            "13   490001157K                      0             0            0    0\n",
            "14   490007657T                      0             0            0    0\n",
            "15   490009293S                      0             0            0    0\n",
            "16   490009704A                      0             0            0    0\n",
            "17   490000019B                      0             0            0    0\n",
            "18   490008035F                      0             0            0    0\n",
            "19   490003565W                      0             0            0    0\n",
            "No arrival predictions available.\n",
            "Refreshing data in 30 seconds...\n",
            "\n",
            "No arrival predictions available.\n",
            "Refreshing data in 30 seconds...\n",
            "\n",
            "No arrival predictions available.\n",
            "Refreshing data in 30 seconds...\n",
            "\n",
            "No arrival predictions available.\n",
            "Refreshing data in 30 seconds...\n",
            "\n",
            "No arrival predictions available.\n",
            "Refreshing data in 30 seconds...\n",
            "\n",
            "No arrival predictions available.\n",
            "Refreshing data in 30 seconds...\n",
            "\n",
            "No arrival predictions available.\n",
            "Refreshing data in 30 seconds...\n",
            "\n",
            "No arrival predictions available.\n",
            "Refreshing data in 30 seconds...\n",
            "\n",
            "No arrival predictions available.\n",
            "Refreshing data in 30 seconds...\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-ca0fd010080b>\u001b[0m in \u001b[0;36m<cell line: 134>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-ca0fd010080b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Refreshing data in 30 seconds...\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;31m# Update cumulative dataframe in dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOWOj7EdsUzrf6UmE5nwZC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}